---
title: 'Lab 15: Final Project'
author: "Brian Teklits, Charles Doremieux, Andrew MacLean, Clint LaBattaglia"
date: "12/16/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE,message=FALSE)
library(modelr)
library(tidyverse)
library(maps)
library(lubridate)
# install.packages("openintro")
library(openintro)
# install.packages("noncensus")
library(noncensus)
# Data from: https://www.kaggle.com/sobhanmoosavi/us-accidents
# LARGE: do not upload to github
accidents <- read_csv("US_Accidents_May19.csv")
```

## Main Question

**Domain Expert and Dataset:**

The domain expert is the NHTSA, the National Highway Traffic Safety Administration.

The dataset we are using can be found [here.](https://www.kaggle.com/sobhanmoosavi/us-accidents) It contains 2.25 million records of accidents from 49 different states, starting in February 2016 until March 2019. It includes data on the severity of the accident, where and when the accident occurred, the weather at the time of the accident, and various information about the nearby surroundings. 

**Main Question & it's Importance:**

What are the overall demographics of the dataset? Can we identify places which are more prone to severe accidents than others?

These questions help us get a better understanding of the overall trends of the dataset. Then, we can attempt to identify problem areas which warrant action or further investigation as to why they are accident prone.

#### Overall Demographics

The dataset contains longitude and latitude data for each accident. The total records of accidents by state:
```{r}
acc_count <- accidents %>%
  group_by(State) %>%
  summarize(total_acc = n()) %>%
  mutate(region = tolower(abbr2state(State)))

united_states <- map_data("state")
overall <- united_states %>%
  full_join(acc_count, by = "region")

ggplot(overall, aes(x=long,y=lat,group=group))+
  geom_polygon(aes(fill=total_acc),color = "black")+
  scale_fill_gradientn("Accidents",colours=rev(heat.colors(10)),na.value="grey90")+
  coord_quickmap()+
  ggtitle("Number of Accidents by State")+
  xlab("Longitude")+
  ylab("Latitude")
```

The number of accidents seems to follow overall population trends, with California, Texas and Florida, the three largest states by population, producing the most accidents.

The dataset provides a few different ways to measure the severity of accidents. Here, we will focus on the severity measure, which describes the duration of impact of an accident on traffic flow. Severity ranges from 0 to 4, with 0 having no impact on traffic and 4 having a long lasting impact.

The overall breakdown of severity:
```{r}
accidents %>%
  group_by(Severity) %>%
  count()
```

We see very few accidents of 0 or 1 severity: less than a thousand out of 2.2 million total. It seems reasonable to conclude that very minor accidents are resolved without the need to report, not that they do not occur often. We'll focus on accidents with a severity of 4, as these represent the top 3% of longest delays: reducing occurances of these accidents will increase road safety and improve overall traffic flows.

#### Severe Accidents

```{r}
sev_count <- accidents %>%
  group_by(State) %>%
  filter(Severity == 4) %>%
  summarize(total_acc = n()) %>%
  mutate(region = tolower(abbr2state(State)))

sev_4 <- united_states %>%
  full_join(sev_count, by = "region")

ggplot(sev_4, aes(x=long,y=lat,group=group))+
  geom_polygon(aes(fill=total_acc),color = "black")+
  scale_fill_gradientn("Accidents",colours=rev(heat.colors(10)),na.value="grey90")+
  coord_quickmap()+
  ggtitle("Number of Severe Accidents by State")+
  xlab("Longitude")+
  ylab("Latitude")
```

The top 10 in total severe accidents:
```{r}
sev_count %>%
  arrange(desc(total_acc))
```
While overall accident occurances generally followed population trends, severe accidents break this trend in a few notable ways. Texas, the second most populous state, has the 10th most severe accidents, while Maryland, the 19th most populous state, ranks 8th. This suggests Maryland is a potential state which can make improvements in road safety.

Looking at the accidents in Maryland in detail:
```{r}
md_s <- accidents %>%
  filter(State == "MD"& Severity == 4)
md <- map_data("state", region ="maryland")
bmore <- tibble(lat = 39.2904, long = -76.6122)
dc <- tibble(lat = 38.9072, long = -77.0369)

ggplot()+
  geom_polygon(data = md, aes(long,lat,group = group), fill = "white",color = "black")+
  coord_quickmap()+
  geom_point(data = md_s, aes(Start_Lng,Start_Lat), alpha=.05)+
  geom_point(data=bmore, aes(long, lat, color = "Baltimore"), size = 2)+
  geom_point(data = dc, aes(long,lat, color = 'Washington DC'),size = 2)+
  scale_color_manual(name = "", values = c("Baltimore" = "red",`Washington DC` = "blue"))+
  ggtitle("Severe Accidents in Maryland")+
  xlab("")+
  ylab("")
```

Accidents tend to cluster around the two main metropolitan areas and along major highways. 

We've found reason to suggest that population is not the only factor at play when looking at severe accidents. In order to verify this claim, we can build a model to predict the number of severe accidents using population and examine its predictive power. The dataset doesn't contain information about population, so we must add it from another source. We used the noncensus package to get 2010 census population data.

```{r}
data(states)
with_pop <- accidents %>%
  full_join(states,by = c("State" = "state"))

test_data <- with_pop %>%
  group_by(population,State) %>%
  filter(Severity == 4) %>%
  summarize(accident_count = n()) %>%
  filter(accident_count > 50) # dropping states with very low number of observations
test_data$population <- as.numeric(test_data$population)

pop_mod <- lm(accident_count ~ population, data = test_data)
mod_pred <- test_data %>% add_predictions(pop_mod)
mod_resid <- test_data %>% add_residuals(pop_mod)
```

Looking at the residuals for our model:
```{r}
ggplot(mod_resid,aes(population,resid))+
  geom_line()
```

The model does a poor job overall, with large residuals across the full range of population sizes. Looking at the states with the largest (positive) residuals:
```{r}
mod_resid %>%
  arrange(desc(resid))
```

Most of the states with the largest number of severe accidents are poorly predicted by the model. One notable exception is California, which the model predicts fairly accurately. In particular, the model doesn't predict Maryland accurately, supporting our claim that accident occurances in Maryland are not related to population.

**Recommendation:**

Based on these findings, we recommend additional research into Maryland road conditions and traffic patterns. We're not able to pinpoint why Maryland roads have an elevated rate of severe accidents, but evidence suggests improvements can be made.

## Individual Sections

### Andrew

  Questions Ideas: For the final project I decided I wanted to check if temperature played a part in accident duration as well as severity. I wanted to use accident duration  in order to plot my data to get a better understanding of it on a continous scale. In addition, I wanted to check if either of these two factors play a part in severity.
  My first step was to change the difference between the start and end times into units of minutes. I did this in order to get a better understanding about exactly whats happening.
```{r}
andrewsData <- accidents %>%
  mutate(eTime = ymd_hms(End_Time)) %>%
  mutate(sTime = ymd_hms(Start_Time)) %>%
  mutate(accidenTime = difftime(eTime,sTime , units = "mins"))
```
The next step was to go though and clean the data.
```{r}
andrewsData <- andrewsData%>%
  filter( !is.na(accidenTime),!is.na(`Temperature(F)`)) %>%
  filter(accidenTime >= 0 & accidenTime <= 4400) %>%
  select(sTime, eTime, accidenTime, `Temperature(F)`, `Severity`)

```
Then I wanted to see the number of accidents per severity when there is a known temperature. The reason for this is to give myself a hint for how I think this question will turn out. I know now that a large majority of the accidents are of the lowest severity. The lowest severity is also the majority of the dataset.
```{r}
ggplot(data = andrewsData, mapping = aes(x = `Severity`, y = `Temperature(F)` ))+
  geom_col()+
  ggtitle("Number of Accidents per Severity Level (w/ Known Temperature)")+
  xlab("Severity")+
  ylab("Number of Accidents")+
  xlim(c(1,5))
```
Next, I wanted to create a linear model in order to better understand the data I am looking out.
```{r}
timeModelTemp <- lm(`Temperature(F)` ~ andrewsData$accidenTime, data = andrewsData)
```
After, I created predictions which I will later compare to the linear model.
```{r}
predsTemp <- andrewsData %>%
  data_grid(`Temperature(F)` = seq_range(`Temperature(F)`, 2181028))%>%
  add_residuals(timeModelTemp, 'reds')%>%
  add_predictions(timeModelTemp,'preds') 
```
Now that I have all the data I need, it is time to model and find some conclusions.
```{r}
ggplot(data = andrewsData, mapping = aes(x = `accidenTime`, y = `Temperature(F)`, color =  `Severity`))+
  geom_point()+
  ggtitle("Temperature as a Result of Accident Time")+
  xlab("Time in Minutes")+
  ylab("Temparature")
```

```{r}
ggplot(data = predsTemp, mapping = aes(x = `preds`, y = `Temperature(F)`))+
  geom_point()+
  ggtitle("Predictions of Temperature versus Accident Time")+
  xlab("Time in Minutes")+
  ylab("Temperature")
```
Although this is a prediction graph, it is interesting to see that it seems when the temperature is higher, there is morw likely chnce that the accident will have a longer duration.
```{r}
ggplot(data = predsTemp, mapping = aes(x = `reds`, y = `Temperature(F)`))+
  geom_line()+
  ggtitle("Residuals of Temperature versus Accident Time")+
  xlab("Time in Minutes")+
  ylab("Residuals")
```
After looking at the predictions against the actual residuals, it is interesting to see that both suggest that there are more accidents at higher temperatures. Along with that conclusion, it seems that accidents at higher temperatures also take longer. Now we can conclude that higher temperatures do play a large part in the duration and amount of accidents.

### Brian

**Question:** What regions of the country are prone to serious accidents? 

By investigating where serious accidents occur, we can attempt to find roads/regions where efforts to improve road safety should be improved. This question is a followup to the team question, where we attempt to perform a similar analysis using a different metric to measure the severity of an accident.

The dataset contains Traffic Message Codes (TMC) for each recorded accident. There are thousands of codes, which can be viewed [here](https://wiki.openstreetmap.org/wiki/TMC/Event_Code_List); to answer this question, I'll use the following two codes:

* 202	(Q) serious accident(s)		
* 203	multi-vehicle accident (involving Q vehicles)

The dataset doesn't contain the Q variable referenced, but we can still use these codes to pinpoint accidents which involved multiple vehicles, or were deemed serious.

To get an idea of what regions to focus on, I opted to look at the state level. For each state, we look at what proportion of total accidents were serious. 
```{r}
serious <- accidents %>%
  filter(TMC == 202|TMC == 203) %>%
  group_by(State) %>%
  count() %>%
  mutate(region = tolower(abbr2state(State))) # openintro package function to convert abbreviation names to full state name
count <- accidents %>%
  group_by(State) %>%
  summarize(total_accidents = n())

serious_accidents <- united_states %>%
  full_join(serious, by = "region") %>%
  full_join(count) %>%
  mutate(prop = n/total_accidents)
```

We can use the longitude and latitude data to plot this on a map:
```{r}
ggplot(serious_accidents, aes(x=long,y=lat,group=group))+
  geom_polygon(aes(fill=prop),color = "black")+
  scale_fill_gradientn(colours=rev(heat.colors(10)),na.value="grey90")+
  coord_quickmap()+
  ggtitle("Proportion of Serious Accidents by State")+
  xlab("Longitude")+
  ylab("Latitude")
```

Massachusetts stands out as an outlier, with almost 6% of accidents being serious. We also note there are 6 states with 0 recorded serious accidents, suggesting the use of TMC codes may be inconsistent across regions.

Looking specifically at Massachusetts:
```{r}
ma_s <- accidents %>%
  filter(State == "MA"& (TMC == 202|TMC == 203))
ma <- map_data("state", region ="massachusetts")
boston <- tibble(lat = 42.3601, long = -71.0589)

ggplot()+
  geom_polygon(data = ma, aes(long,lat,group = group), fill = "white",color = "black")+
  coord_quickmap()+
  geom_point(data = ma_s, aes(Start_Lng,Start_Lat), alpha=.05) +
  geom_point(data = boston, aes(long,lat,color="Boston"), size = 3)+
  scale_color_manual(name = "", values = c("Boston" = "red"))+
  ggtitle("Serious Accident Occurances in MA")+
  xlab("")+
  ylab("")
```

We see that serious accidents are clustered around Boston. Before investigating this further, we'll run a permutation test on the proportion of serious accidents in Massachusetts, to get an idea of the significance of our result. 

The null hypothesis is that there is no difference in proportion of serious accidents between the proportion of serious accidents in Massachusetts and the nationwide proportion of serious accidents. The alternate hypothesis is that there is a difference between the two.

First, we'll need the national proportion of serious accidents:
```{r}
total_count <- accidents %>%
  count()
ser_count <- accidents %>%
  filter(TMC == 202|TMC == 203) %>%
  count() 
ser_count/total_count
```

Only .8% of accidents nationwide are categorized as serious.
Our actual proportion of serious accidents in Massachusetts:
```{r}
MA_count <- accidents %>%
  filter(State == "MA") %>%
  count()
MA_serious <- accidents %>%
  filter(TMC == 202|TMC == 203) %>%
  filter(State == "MA") %>%
  count() 
(MA_actual <- MA_serious/MA_count)
```
```{r}
ma_data <- accidents %>%
  filter(State == "MA") %>%
  count()
perm_test <- numeric(1000)
for(i in seq_along(perm_test))
{
  samp <- sample_n((accidents), ma_data$n) # random sample the same size as massachusetts data
  samp_ser <- samp %>%
    filter(TMC == 202 | TMC == 203) %>%
    summarize(serious = n()) %>%
    mutate(prop = serious/ma_data$n)
  perm_test[i] <- samp_ser$prop
}

ggplot(tibble(perm_test),aes(perm_test))+
  geom_histogram(bins = 50)+
  geom_vline(xintercept = MA_actual$n, color = "red")+
  ggtitle("Permutation Test for Proportion of Serious Accidents")+
  xlab("Proportion")+
  ylab("Number of Tests")
```

```{r}
sum(perm_test > MA_actual)/length(perm_test)
```
Our result is in the 99th percentile of results, suggesting we have a significant enough result to reject the null hypothesis, and assert that Massachusetts has a different proportion of serious accidents from the national average.

Since our result is significant, we're left to ask: Why does Boston have a much larger rate of serious accidents compared to other major cities? If there was nothing special about Boston, we might expect to see similar proportions around other major cities.

Answering this question requires additional data, so I have two recommendations. First, investigate the usage of TMC codes across states. We had some evidence to suggest TMC codes are not widely used in some states, so one explanation is that Massachusetts uses TMC codes more than other states, and thus appears to have a higher rate of serious accidents as a result. 

Second, if we can't explain these results with TMC code usage, then additional research should be done. Cities similar to Boston in a variety of ways (layout, demographics) should be determined, and then investigated similarly as we did here. If these cities don't have similar accident rates, then we have a solid basis to recommend changes. Either Boston roads or drivers (or both) are more dangerous than the typical driver. Both road improvements and driver education programs should be considered to reduce the frequency of serious accidents.

### Charles

My goal is to determine if there is a positive relationship between Humidity and the duration of an accident. My intuition is that the more humidity, the less traction there will be on the road, shortening the length of an accident. The duration of an accident can help suggest the severity of the accident and the effect it will have on the flow of traffic. My expectation is that if I can demonstrate that Humidity is positively or negatively associated with accident duration, I can make a determination on which states are the most likely to have short or long accidents.

First I need to create a model of the time an accident takes and seperate outlier values.
```{r}
cd_data <- accidents %>%
  mutate(a_start = ymd_hms(Start_Time)) %>%
  mutate(a_end = ymd_hms(End_Time)) %>%
  mutate(accident_time = difftime(a_end, a_start, units = "mins")) %>%
  filter(!is.na(`Humidity(%)`), !is.na(accident_time)) %>%
  filter(accident_time >= 0 & accident_time <= 4320) %>%
  select(a_start, a_end, accident_time, `Humidity(%)`)
```

The model needs to include predictions and residuals.
```{r}
humid_time_model <- lm(cd_data$accident_time ~ `Humidity(%)`, data = cd_data)

humid_preds <- cd_data %>%
  data_grid(`Humidity(%)` = seq_range(`Humidity(%)`, 2178823)) %>%
  add_predictions(humid_time_model,'predictions') %>%
  add_residuals(humid_time_model, 'residuals')
```

The view of the raw data.
```{r}
ggplot() +
  geom_point(data = cd_data, aes(`Humidity(%)`,accident_time))+
  ggtitle("View of Humidity as it Relates to Accident Duration")+
  xlab("Humidity (%)")+
  ylab("Accident Time (minutes)")
```

Predictions of the data.
```{r}
ggplot()+
  geom_line(data = humid_preds, aes(`Humidity(%)`, humid_preds$predictions))+
  ggtitle("Predictions of Humidities Related to Accident Duration")+
  xlab("Humidity (%)")+
  ylab("Accident Time (minutes)")
```

```{r}
ggplot()+
  geom_line(data = humid_preds, aes(`Humidity(%)`, humid_preds$residuals))+
  ggtitle("Residuals of Humidities Related to Accident Duration")+
  xlab("Humidity (%)")+
  ylab("Accident Time (minutes)")
```

```{r}
# perm_mean <- function(perms = 1000, x, y)
# {
#   perm_mean_diffs2 <- numeric(perms)
#   
#   for(i in c(1:perms))
#   {
#     rand_order_y <- sample(y)
#     cor_comp <- cor(x,rand_order_y)
#     perm_mean_diffs2[i] <- cor_comp
#   }
#   return(perm_mean_diffs2)
# }
# 
# cc <- perm_mean(1000, cd_data$`Humidity(%)`, as.numeric(cd_data$accident_time))

```

The model confirms my suspicion that as the Humidity(%) increases the length of an accident shortens, though the model is not absolute in its certainty. Nevertheless there is a general trend to shorter accidents when Humidity is high. The model suggests Humidity can be used to predict the duration of an accident by: -0.2375825(Humidity) with the intercept 111.0417.

### Clint

Question Ideas: Explore relationship between severity of accidents and stoplights + percentage of accidents with each of the binary variables. 
uses map functino to take percentiles if true false sections and graph + permutation test to see if there is correlation between severity of accident and presence of stoplight or just most common accident indicator +
Question: Which of the true/false variables is the greatest predictor of an accident, and are any of them related to a the probability of a more severe accident? This question is important/interesting because if a majority of accidents occur near specific locations, like a traffic signal or railway crossing then  countermeassures like positioning police vehicles nearby or placing extra warning signs in the area can be put in place which may save lives. Also if some presences are especially indicative of more severe accidents, those area could be targeted for increased safty measures. 

First I want to examine the percentage of total accidents associated with each of the true/false columns so  created a for loop which calculated the percentage of total accidents which had a true response for each of the columns and stored it in a dataframe called sums_true.
```{r}
library(knitr)
explore <- accidents%>%
  select(Amenity, Bump, Crossing, Give_Way, Junction, No_Exit, Railway, Roundabout, Station, Stop, Traffic_Calming, Traffic_Signal, Turning_Loop)

sums_true <- data.frame(mode = "double", ncol(explore))
for (i in seq_along(explore)) {
  sums_true[i] <- (sum(explore[[i]])/tally(accidents))*100
}
```
From there I can see that eight of the thirteen variables are true in less than 1 percent of observed accidents so, while that doesn't make them negligable, I would prefer to focus on the more significant variables. It is also worth noting that the total of all percentages added together is 34.66% which means 65.34% of accidents didn't have any of the true/false variable associated with them.  

```{r}
ggplot(data = sums_true) + geom_col(aes(x= 1, y = sums_true$n.9, fill = "Traffic Signal"))+
  geom_col(aes(x= 2, y = sums_true$n.2, fill = "Junction")) +
  geom_col(aes(x = 3, y = sums_true$n, fill = "Crossing"))+
  geom_col(aes(x = 4, y = sums_true$n.6, fill = "Station"))+
  geom_col(aes(x = 5, y = sums_true$mode, fill = "Amenity"))+
  ggtitle("Top Five Presence at Accident Percentages")+
  ylab("Percent of Accidents Present")+
  xlab("Rank")
```
So I now know that the three highest presences by far are traffic signals, junctions, and crossings. Now I'm curious if any of these variables have an effect on the average severity of an accident. 
```{r}
traffic_true<- accidents%>%
  filter(Traffic_Signal == "TRUE")
Junction_true <-accidents%>%
  filter(Junction == "TRUE")
crossing_true <-accidents%>%
  filter(Crossing == TRUE)
without_boolean <- accidents%>%
  filter(Amenity == FALSE, Bump == FALSE, Crossing  == FALSE, Give_Way == FALSE, Junction == FALSE, No_Exit == FALSE, Railway == FALSE, Roundabout == FALSE, Station == FALSE, Stop == FALSE, Traffic_Calming == FALSE, Traffic_Signal == FALSE, Turning_Loop == FALSE)

mean1 <- mean(accidents$Severity)
mean2 <-mean(traffic_true$Severity)
mean3 <-mean(Junction_true$Severity)
mean4 <-mean(crossing_true$Severity)
mean5 <-mean(without_boolean$Severity)
```

```{r}
ggplot()+ geom_col(aes(x= 1, y = mean3, fill = "Junction")) +
  geom_col(aes(x = 2, y = mean5, fill = "Average severity without True/False"))+
  geom_col(aes(x= 3, y = mean1, fill = "Unfiltered Average Severity"))+
  geom_col(aes(x=4, y = mean2, fill = "Traffic Signal" ))+
  geom_col(aes(x=5, y=mean4, fill = "Crossing"))+
  ggtitle("Average Severity Across Top Three Acident Presences")+
  ylab("Mean Severity")+
  xlab("Rank")
```

busted function
```{r}
#calc_severity <- function(data, var_name1, var_name2){
#new_data <-  filter(data$var_name1 == "TRUE")
 # mean_severity <- mean(new_data$var_name2)
  #return(mean_severity)
#}
#calc_severity(accidents, accidents$Traffic_Signal, accidents$Severity)
```

New Tools: The primary new tool in R that I used was a for loop in order to  gather the percentages of the total observed accidents with for of the True/False variables. I used a for loop because a map function would not work, I needed to produce a new dataframe with a rowcount of one(the percentage of total accidents) whereas map functions preserve the number of rows. 
Conclusions: From my analysis I can see that the most common indicator is no indicator at all, implying that the majority of accidents either occured in open road not near any features or that the majority of nearby features were not reported in the dataset. However, of the observed indicators, traffic signals, junctions, and crossings accounted for 29.76% of total accidents which is a pretty significant amount. Traffic singals had far and away the highest individual percentage at 15.98%. This leads me to recomend that a public saftey campaign be run that warns people about the dangers of not paying proper attention at traffic signals. Severity is a measure of how impactful the accident was to the flow of traffic, not inherently how destructive a accident is as the name may imply. Consequetially, since Junctions and no observed features had the highest average severity by a fairly large margin I would recommend that machinery and tow trucks used to clear the road after accidents be stored near junctions as this would help to reduce the effect of the accidents on traffic by lowering response time.  

## Reflections on Lab 2

Our team goals were to do well in the course, gain a greater understanding of the use of R and of data science in general, and not make any enemies. We can say with confidence that we accomplised the first two goals. The third may have been a little less successful. If we could travel back in time, there are three things we would do differently. First, we would meet up more often in order to work on the project in a more collaborative fashion rather than individuals doing their sections on their own. Second, we would make more productive use of our time in class as an oppurtunity to deepen our understanding of the material by asking more questions of the profesor. Third, we would communicate more often about not only the labs but the readings to ensure that everyone is at a similar comfort level with new applications and statistical applications. While this was a successful semester for our group in terms of observed grades, there was still room for improvement in the areas of teamwork, communication, and understanding of the material. 

### Individual Reflections

**Andrew:**
After finishing this semester and returning to my goals, I realized how a short amount of time could affect so much about how I looked at my future. My 6 month plan has not changed as I plan to try and take fews years off of school in order to find a job; however, my 5 year plan has changed a lot. I still plan on using statistics but instead of the medical field I have gained interest into astrospheric studies instead. I hope to find a a job in that field because I believe I could make a difference in that field. If I had to give advice to myself I would say to actually read the RIGHT (accidently read the wrong ones for one) chapters for the iRat so I could have helped my team more as well.

**Brian:** My overall goals haven't changed throughout the semester. I learned a great deal throughout the semester, in particular, linear modeling/residuals and permutation tests stand out for me. Methods of quantifying the effectiveness of statistics and analysis are particularly interesting, as prior to learning these there is a lot of "hand wavy" justifications without the concrete mathematical basis to test these justifications. 

I found it helpful to first build an outline for each writeup, with sections and notes on what to include in each section. This process ensured a well structured report, as well as a solid understanding of the needed deliverables for each report. My team in particular may have benefitted more had I contributed slightly less to the overall reports. In taking the lead on the writeups, overall quality was consistent, but my teammates would surely have benefitted from working through more of the material on their own (in my opinion). Though I probably received and ignored this advice at the start of the semester, I recommend staying ahead of the readings, particularly early on in the course. Multiple times throughout the semester I struggled through a problem to eventually reach a patchwork solution only to realize my problem was addressed simply and directly in the upcoming readings.

**Charles:** 
  My original 6-month and five-year year plans have not changed, but my attitudde towards them has certainly changed. I still want to study under some of the greatest voices in philosophy (one of my majors) while working towards some meaningful contribution within my fields of interest. I have been working towards those goals and this semester was invited to take PHIL 6380 (Graduate Metaphysics) by a professor. I have set my eyes on my 5 year goal, and I am very nervous about it. I hope I can achieve what I'm setting out for, though I am nervous I'll fail. I learned a lot about project managing mainly because I've failed a lot this semester. I struggled to manage and process my internal stress and translate it to success externally. These are things I would have liked to work on more. Furthermore, I needed to motivate myself to learn outside of the classroom. I was very dependent on the help of my team-mates and I wish I could have been more independent.

**Clint:**
  Since the beginning of the semester my 6-month to five-year goals have changed in that I have less confidence my ability to thrive in a coding driven profession than I had at the beginnning of the semester. This is useful information to know as I have been considering going into a business masters program in which coding and statistics would feature prominently. Now I am wondering if this is the right path after all. In this course I learned what feels like the basics of programming in R, though I am not a proficcient as a I could or should be, and I also feel as though I learned about the basics of statistics in general which has been really interesting. If I could past me advice I would tell myself to do all the exercises when going through the R4DS textbook, experiment when doing the labs instead of doing the bare minimum, communicate more concisely with my teammates, and go to office hours w/ the professor and TAs.
  
### Individual Contributions

**Andrew:**
I created moidels in order to study severity, Temperature, and times between accidents. I came to the conclusion that there is a large chance of accidents happening in hot weather.

**Brian:** 
I found the dataset, and used the latitude and longitude data to produce the map plots. I found the brief book section on plotting spatial data interesting, so I wanted to make an attempt at plotting map data.

**Charles:** 
  I used various modelr methods to calculate several predictions and residuals for our linear models. I also split data into model building and model validation subsets. By incorporating variable transformations I was able to draw a conclusion on the interactions between Humidity and the period for which an accident lasts.

**Clint:**

