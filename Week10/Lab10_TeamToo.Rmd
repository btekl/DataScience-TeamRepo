---
title: 'Lab 10: Text and Time Analysis'
author: "Brian Teklits, Charles Doremieux, Andrew MacLean, Clint LaBattaglia"
date: "11/01/2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(stringr)
library(lubridate)
knitr::opts_chunk$set(warning=FALSE,message=FALSE)
Que <- read_csv("Questions_trunc.csv")
Ans <- read_csv("Answers_trunc.csv")
Ans <- Ans %>%
  mutate(ans_time = ymd_hms(CreationDate))
Que <- Que %>%
  mutate(ask_time = ymd_hms(CreationDate))
Q_and_A <- Que %>%
  left_join(Ans, by = c("Id" = "ParentId")) %>%
  mutate(time_to_ans = CreationDate.y-CreationDate.x)
Q_and_A <- Q_and_A %>%
  select(-CreationDate.x,-CreationDate.y,-X7.x,-X7.y)
Q_and_A <- Q_and_A %>%
  rename(Score_Que = Score.x, AnswerId = Id.y, Id_Asked = OwnerUserId.x, ID_Answered = OwnerUserId.y, QuestionBody = Body.x, AnswerBody = Body.y, Score_Ans = Score.y)
```
### Findings and Features

**Notable Features:** The tone of a question, ... seem to influence the score of a question. 

Answers which include examples, ... influence the score of a submission.

**Ideal question and answer:** The ideal question should not be demanding, ...

The ideal answer should be specific, with examples, ...

#### Timeliness as a Feature

First, the timeliness variable must be created. The time_to_ans variable describes the number of minutes between the posting of a question and an answer. Answers are ranked in order of response:
```{r}
Q_and_A2 <- Q_and_A %>%
  group_by(Id) %>%
  mutate(Timeliness = min_rank(time_to_ans))
```
Most questions receive only a few answers, so the earliest answer ranks are the most crowded:
```{r}
(Ans_per_Timeli <- Q_and_A2 %>%
  group_by(Timeliness) %>%
  count())
```
So, to compare across ranks, we compare the average score at each rank:
```{r}
Score_per_Ans <- Q_and_A2 %>%
  group_by(Timeliness) %>%
  summarize(Score_per_Ans = mean(Score_Ans))
```

```{r}
ggplot()+
geom_line(data=Score_per_Ans,aes(Timeliness,Score_per_Ans))+
  geom_hline(aes(yintercept=mean(Q_and_A2$Score_Ans,na.rm=TRUE),color='Sample Average'),linetype='dashed')+
  scale_color_manual(name='',values=c(`Sample Average`='red'))+
  ggtitle("Average Answer Score by Response Order")+
  ylab("Average Score")+
  xlab("Answer in Order of Response")
```

Earlier responses don't outscore later responses, in general they fall very close to the overall average score. Some of the late responses score far above the average: questions which generate a lot of responses are more nuanced and it takes longer for a person with the adequate knowledge to respond. Additionally, only 4 questions received more than 20 responses, so the averages at these ranks are heavily weighted by any high scoring response.

#### Is answer score normally distributed?

```{r}
ggplot(Q_and_A2,aes(Score_Ans))+
  geom_histogram(bins = 100)+
  geom_vline(aes(xintercept=mean(Q_and_A2$Score_Ans,na.rm=TRUE),color='Mean'),linetype='dashed')+
  geom_vline(aes(xintercept=median(Q_and_A2$Score_Ans,na.rm=TRUE),color='Median'),linetype='dashed')+
  scale_color_manual(name='Statistics',values=c(Mean='red',Median='blue'))+
  xlim(-3,25)+
  ggtitle("Distribution of Answer Scores")+
  ylab("Number of Answers")+
  xlab("Score")
```

Score is not normally distributed; a normal distribution is centered on the mean. The presence of extreme outliers (one question received a score of 8384) skews the mean away from where the majority of answer scores fall. We are able to compute percentiles and z-scores, but they will be similarly skewed, and thus not as evenly spaced as for a normal distribution.

**Benefits of this analysis:** Recommendations for question and answer styling could be incorporated into a Stack Overflow beginner's guide.

### Individual Findings

#### Andrew
What features did you create and why did you choose those features?
What is the relationship between your features and the scores of questions/answers if any? Provide an explanation for why each relationship does/does not exist.

#### Brian

For questions, I created a feature related to the number of mentions of the word "need". I figured questions that came off as demanding would be scored lower overall, and tried features with a variety of words that could come across as demanding.

For answers, my feature checks if a submission included the code tag `<code>`. I reasoned that including specific code examples would generally increase an answers score as it's something I've found helpful when using Stack Overflow.

Creating the factor for questions:
```{r}
bq <- Q_and_A2 %>%
  mutate(need = str_count(QuestionBody,regex("need",ignore_case = TRUE)))

bq2 <- bq %>%
  group_by(need) %>%
  summarize(med = median(Score_Que,na.rm = TRUE)) 

bq3 <- bq %>%
  group_by(need) %>%
  summarize(mean = mean(Score_Que,na.rm = TRUE))

ggplot()+
  geom_point(data=bq3,aes(need,mean,color="Mean"))+
  geom_smooth(data=bq3,aes(need,mean,color="Mean"),se=FALSE,method='lm',linetype="dashed")+
  geom_point(data=bq2,aes(need,med,color="Median"))+
  geom_smooth(data=bq2,aes(need,med,color="Median"),se=FALSE,method='lm',linetype="dashed")+
  scale_color_manual(name = "",values = c(Mean = "red",Median="black"))+
  ggtitle("Mean and Median Scores by Usage of 'Need'")+
  ylab("Score")+
  xlab("Number of Usages of 'Need'")
```

The mean score is substantially lower for higher usages of need, while the median score appears to be unaffected. It is difficult to say for sure that there is a relationship between mentions of need and score, but the affect on average score gives some evidence of a link between them. 

Questions which seem to be demanding or forcing a sense of urgency can be off putting for a person answering the question for free on their own time.

Creating the factor for answers:
```{r}
ba <- Q_and_A2 %>%
  mutate(Inc_Code = str_detect(AnswerBody,"<code>")) %>%
  mutate(s = ifelse(Score_Ans>=41, "High", "Low"))
ba2 <- ba %>%
  filter(!is.na(Inc_Code)& s=="High") %>%
  group_by(Inc_Code)%>%
  summarize(mean = mean(Score_Ans,na.rm=TRUE))
ba3 <- ba %>%
  filter(!is.na(Inc_Code)& s=="Low") %>%
  group_by(Inc_Code)%>%
  summarize(mean = mean(Score_Ans,na.rm=TRUE))
```
Scores are grouped into high and low, with high scores being the top 5%. Because the scales are so different, they are plotted seperately:
```{r}
ggplot(ba2,aes(Inc_Code,mean,fill=Inc_Code))+
  geom_col()+
  ggtitle("Affect of Code on High Score Answers")+
  ylab("Average Score")+
  xlab("Usage of Code in Answer")+
  theme(legend.position = "none")
ggplot(ba3,aes(Inc_Code,mean,fill=Inc_Code))+
  geom_col()+
  ggtitle("Affect of Code on Low Score Answers")+
  ylab("Average Score")+
  xlab("Usage of Code in Answer")+
  theme(legend.position = "none")
```

Including code seems to have a notable affect, both on answers that performed well and those that did poorly. Answers using code examples score nearly twice as well in both groups. 

Code examples are likely linked to specific parts of a question which are the most relevant. So, inclusion of code may represent more direct answers than those without code.

#### Charles
What features did you create and why did you choose those features?
What is the relationship between your features and the scores of questions/answers if any? Provide an explanation for why each relationship does/does not exist.

#### Clint
What features did you create and why did you choose those features?
What is the relationship between your features and the scores of questions/answers if any? Provide an explanation for why each relationship does/does not exist.
