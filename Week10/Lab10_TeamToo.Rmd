---
title: 'Lab 10: Text and Time Analysis'
author: "Brian Teklits, Charles Doremieux, Andrew MacLean, Clint LaBattaglia"
date: "11/01/2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(stringr)
library(lubridate)
knitr::opts_chunk$set(warning=FALSE,message=FALSE)
Que <- read_csv("Questions_trunc.csv")
Ans <- read_csv("Answers_trunc.csv")
Ans <- Ans %>%
  mutate(ans_time = ymd_hms(CreationDate))
Que <- Que %>%
  mutate(ask_time = ymd_hms(CreationDate))
Q_and_A <- Que %>%
  left_join(Ans, by = c("Id" = "ParentId")) %>%
  mutate(time_to_ans = CreationDate.y-CreationDate.x)
Q_and_A <- Q_and_A %>%
  select(-CreationDate.x,-CreationDate.y,-X7.x,-X7.y)
Q_and_A <- Q_and_A %>%
  rename(Score_Que = Score.x, AnswerId = Id.y, Id_Asked = OwnerUserId.x, ID_Answered = OwnerUserId.y, QuestionBody = Body.x, AnswerBody = Body.y, Score_Ans = Score.y)
```
### Findings and Features

**Notable Features:** The tone of a question, ... seem to influence the score of a question. 

Answers which include examples, ... influence the score of a submission.

**Ideal question and answer:** The ideal question should not be demanding, ...

The ideal answer should be specific, with examples, ...

#### Timeliness as a Feature

First, the timeliness variable must be created. The time_to_ans variable describes the number of minutes between the posting of a question and an answer. Answers are ranked in order of response:
```{r}
Q_and_A2 <- Q_and_A %>%
  group_by(Id) %>%
  mutate(Timeliness = min_rank(time_to_ans))
```
Most questions receive only a few answers, so the earliest answer ranks are the most crowded:
```{r}
(Ans_per_Timeli <- Q_and_A2 %>%
  group_by(Timeliness) %>%
  count())
```
So, to compare across ranks, we compare the average score at each rank:
```{r}
Score_per_Ans <- Q_and_A2 %>%
  group_by(Timeliness) %>%
  summarize(Score_per_Ans = mean(Score_Ans))
```

```{r}
ggplot()+
geom_line(data=Score_per_Ans,aes(Timeliness,Score_per_Ans))+
  geom_hline(aes(yintercept=mean(Q_and_A2$Score_Ans,na.rm=TRUE),color='Sample Average'),linetype='dashed')+
  scale_color_manual(name='',values=c(`Sample Average`='red'))+
  ggtitle("Average Answer Score by Response Order")+
  ylab("Average Score")+
  xlab("Answer in Order of Response")
```

Earlier responses don't outscore later responses, in general they fall very close to the overall average score. Some of the late responses score far above the average: questions which generate a lot of responses are more nuanced and it takes longer for a person with the adequate knowledge to respond. Additionally, only 4 questions received more than 20 responses, so the averages at these ranks are heavily weighted by any high scoring response.

#### Is answer score normally distributed?

```{r}
ggplot(Q_and_A2,aes(Score_Ans))+
  geom_histogram(bins = 100)+
  geom_vline(aes(xintercept=mean(Q_and_A2$Score_Ans,na.rm=TRUE),color='Mean'),linetype='dashed')+
  geom_vline(aes(xintercept=median(Q_and_A2$Score_Ans,na.rm=TRUE),color='Median'),linetype='dashed')+
  scale_color_manual(name='Statistics',values=c(Mean='red',Median='blue'))+
  xlim(-3,25)+
  ggtitle("Distribution of Answer Scores")+
  ylab("Number of Answers")+
  xlab("Score")
```

Score is not normally distributed; a normal distribution is centered on the mean. The presence of extreme outliers (one question received a score of 8384) skews the mean away from where the majority of answer scores fall. We are able to compute percentiles and z-scores, but they will be similarly skewed, and thus not as evenly spaced as for a normal distribution.

**Benefits of this analysis:** Recommendations for question and answer styling could be incorporated into a Stack Overflow beginner's guide.

### Individual Findings

#### Andrew
My feature count the number of questions that include the string "integer" in some form. This feature is created in order to view the importance of this program to people/organization asking the questions. This will help us to analyze how important this concept is in the field.

```{r}

int <- Q_and_A2%>%
  mutate(integer = str_count(QuestionBody, regex("integer", ignore_case =  TRUE)))


medint <- int %>%
  group_by(integer) %>%
  summarize(med = median(Score_Que,na.rm = TRUE)) 

meanint <- int %>%
  group_by(integer) %>%
  summarize(mean = mean(Score_Que,na.rm = TRUE))


ggplot(meanint, mapping = aes( x= integer,y = mean, fill = integer ))+
  geom_col()

ggplot(medint, mapping = aes( x= integer,y = med, fill = integer ))+
  geom_col()+
  scale_fill_gradient()
```



```{r}

```

#### Brian

For questions, I created a feature related to the number of mentions of the word "need". I figured questions that came off as demanding would be scored lower overall, and tried features with a variety of words that could come across as demanding.

For answers, my feature checks if a submission included the code tag `<code>`. I reasoned that including specific code examples would generally increase an answers score as it's something I've found helpful when using Stack Overflow.

Creating the factor for questions:
```{r}
bq <- Q_and_A2 %>%
  mutate(need = str_count(QuestionBody,regex("need",ignore_case = TRUE)))

bq2 <- bq %>%
  group_by(need) %>%
  summarize(med = median(Score_Que,na.rm = TRUE)) 

bq3 <- bq %>%
  group_by(need) %>%
  summarize(mean = mean(Score_Que,na.rm = TRUE))

ggplot()+
  geom_point(data=bq3,aes(need,mean,color="Mean"))+
  geom_smooth(data=bq3,aes(need,mean,color="Mean"),se=FALSE,method='lm',linetype="dashed")+
  geom_point(data=bq2,aes(need,med,color="Median"))+
  geom_smooth(data=bq2,aes(need,med,color="Median"),se=FALSE,method='lm',linetype="dashed")+
  scale_color_manual(name = "",values = c(Mean = "red",Median="black"))+
  ggtitle("Mean and Median Scores by Usage of 'Need'")+
  ylab("Score")+
  xlab("Number of Usages of 'Need'")
```

The mean score is substantially lower for higher usages of need, while the median score appears to be unaffected. It is difficult to say for sure that there is a relationship between mentions of need and score, but the affect on average score gives some evidence of a link between them. 

Questions which seem to be demanding or forcing a sense of urgency can be off putting for a person answering the question for free on their own time.

Creating the factor for answers:
```{r}
ba <- Q_and_A2 %>%
  mutate(Inc_Code = str_detect(AnswerBody,"<code>")) %>%
  mutate(s = ifelse(Score_Ans>=41, "High", "Low"))
ba2 <- ba %>%
  filter(!is.na(Inc_Code)& s=="High") %>%
  group_by(Inc_Code)%>%
  summarize(mean = mean(Score_Ans,na.rm=TRUE))
ba3 <- ba %>%
  filter(!is.na(Inc_Code)& s=="Low") %>%
  group_by(Inc_Code)%>%
  summarize(mean = mean(Score_Ans,na.rm=TRUE))
```
Scores are grouped into high and low, with high scores being the top 5%. Because the scales are so different, they are plotted seperately:
```{r}
ggplot(ba2,aes(Inc_Code,mean,fill=Inc_Code))+
  geom_col()+
  ggtitle("Affect of Code on High Score Answers")+
  ylab("Average Score")+
  xlab("Usage of Code in Answer")+
  theme(legend.position = "none")
ggplot(ba3,aes(Inc_Code,mean,fill=Inc_Code))+
  geom_col()+
  ggtitle("Affect of Code on Low Score Answers")+
  ylab("Average Score")+
  xlab("Usage of Code in Answer")+
  theme(legend.position = "none")
```

Including code seems to have a notable affect, both on answers that performed well and those that did poorly. Answers using code examples score nearly twice as well in both groups. 

Code examples are likely linked to specific parts of a question which are the most relevant. So, inclusion of code may represent more direct answers than those without code.

#### Charles
For the questions I'm curious to see if the quality of an answer is dependent upon the inclusion of punctuation and capitalization. I suspect the better a question is written the quicker it will be answered.

I wondered for answeres if the length of the answer affected was linked to response time. My suspicion is that if a response is longer an answer will come slower, because even complex coding problems are often the result of a formatting error or missing semicolon. Shorter questions have less information, and as such dont make themselves as answerable.

```{r}
quequal <- Q_and_A2 %>%
  mutate(punctuation = str_count(QuestionBody,"[:punct:]|[:upper:]")) %>%
  group_by(punctuation)%>%
  summarize(sq = mean(Score_Que,na.rm = TRUE))
ggplot(data = quequal, aes(punctuation,sq))+
  geom_col()+
  ggtitle("Quality of Questions per Grammatical Unit")+
  xlab("Units of Grammar (capitalization and punctuation)")+
  ylab("Quality")+
  ylim(0,100)+
  xlim(0,100)+
  coord_flip()

howlong <- Q_and_A2 %>%
  mutate(len = nchar(AnswerBody)) %>%
  group_by(len) %>%
  summarize(av = mean(time_to_ans,na.rm = TRUE))
ggplot(data = howlong, aes(len,av))+
  geom_point()+
  geom_smooth()+
  xlim(0,10000)+
  ggtitle("Time Needed to Answer Questions of Differing Lengths")+
  xlab("Character Length for Answer")+
  ylab("Time Before Answer")
```

There seems to be no clear link between the grammar of a question, at least by the metric provided, and the quality of said question. Some shorter questions are considered to be a higher in quality than longer questions and vice versa. There is no clear link. This is probably because including code that is clear is more important than simply wording a question elegantly.

There also seems to be no clear link between the length of an answer and the period of time required to submit it. This must be because of factors like quoting and copy-pastable banks already stored on the responder's console.

#### Clint
What features did you create and why did you choose those features?
What is the relationship between your features and the scores of questions/answers if any? Provide an explanation for why each relationship does/does not exist.
